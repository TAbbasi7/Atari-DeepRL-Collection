{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-x-Cl8wPICj",
        "outputId": "f8307246-8f22-4c85-dd1c-4ae85a9e616c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating environment PongNoFrameskip-v4...\n",
            "Starting Dueling DQN Training (With Real Logs)...\n",
            "Step: 100 / 2000 | Loss: Collecting Data | Epsilon: 0.951\n",
            "Step: 200 / 2000 | Loss: Collecting Data | Epsilon: 0.901\n",
            "Step: 300 / 2000 | Loss: Collecting Data | Epsilon: 0.852\n",
            "Step: 400 / 2000 | Loss: Collecting Data | Epsilon: 0.802\n",
            "Step: 500 / 2000 | Loss: Collecting Data | Epsilon: 0.752\n",
            "Step: 600 / 2000 | Loss: 0.0288 | Epsilon: 0.703\n",
            "Step: 700 / 2000 | Loss: 0.0001 | Epsilon: 0.653\n",
            "Step: 800 / 2000 | Loss: 0.0020 | Epsilon: 0.604\n",
            "Step: 900 / 2000 | Loss: 0.0003 | Epsilon: 0.554\n",
            "Step: 1000 / 2000 | Loss: 0.0038 | Epsilon: 0.505\n",
            "Step: 1100 / 2000 | Loss: 0.0001 | Epsilon: 0.456\n",
            "Step: 1200 / 2000 | Loss: 0.0299 | Epsilon: 0.406\n",
            "Step: 1300 / 2000 | Loss: 0.0001 | Epsilon: 0.357\n",
            "Step: 1400 / 2000 | Loss: 0.0003 | Epsilon: 0.307\n",
            "Step: 1500 / 2000 | Loss: 0.0000 | Epsilon: 0.258\n",
            "Step: 1600 / 2000 | Loss: 0.0001 | Epsilon: 0.208\n",
            "Step: 1700 / 2000 | Loss: 0.0000 | Epsilon: 0.158\n",
            "Step: 1800 / 2000 | Loss: 0.0000 | Epsilon: 0.109\n",
            "Step: 1900 / 2000 | Loss: 0.0000 | Epsilon: 0.059\n",
            "Step: 2000 / 2000 | Loss: 0.0000 | Epsilon: 0.010\n",
            "Dueling DQN Finished.\n"
          ]
        }
      ],
      "source": [
        "# 3_Dueling_DQN.py\n",
        "# Dueling DQN Architecture optimized for Demo Run (With Real Logs)\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import cv2\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# --- DEMO SETTINGS ---\n",
        "env_id = 'PongNoFrameskip-v4'\n",
        "seed = 42\n",
        "lr = 0.0001\n",
        "buffer_size = 50000\n",
        "batch_size = 32\n",
        "warm_start = 500\n",
        "train_freq = 4\n",
        "target_q_update_freq = 200\n",
        "reward_gamma = 0.99\n",
        "number_timesteps = 2000\n",
        "clipnorm = 10.0\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay_steps = 2000\n",
        "\n",
        "# (Reusing same Wrappers for consistency)\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
        "        if terminated or truncated: self.env.reset(**kwargs)\n",
        "        obs, _, terminated, truncated, _ = self.env.step(2)\n",
        "        if terminated or truncated: self.env.reset(**kwargs)\n",
        "        return obs, {}\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3: img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3: img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else: return frame\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        x_t = np.reshape(resized_screen, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=env.observation_space.dtype)\n",
        "    def reset(self, **kwargs):\n",
        "        ob, info = self.env.reset(**kwargs)\n",
        "        for _ in range(self.k): self.frames.append(ob)\n",
        "        return self._get_ob(), info\n",
        "    def step(self, action):\n",
        "        ob, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, terminated, truncated, info\n",
        "    def _get_ob(self):\n",
        "        return np.concatenate(self.frames, axis=2)\n",
        "\n",
        "def build_env(env_id, seed=0):\n",
        "    env = gym.make(env_id, render_mode='rgb_array')\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = FrameStack(env, 4)\n",
        "    env.action_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "    def add(self, obs, act, rew, next_obs, done):\n",
        "        obs = np.array(obs, dtype=np.uint8)\n",
        "        next_obs = np.array(next_obs, dtype=np.uint8)\n",
        "        self.buffer.append((obs, act, rew, next_obs, done))\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, act, rew, next_obs, done = zip(*batch)\n",
        "        return (np.array(obs), np.array(act), np.array(rew, dtype=np.float32), np.array(next_obs), np.array(done, dtype=np.float32))\n",
        "\n",
        "def sync(model, target_model):\n",
        "    target_model.set_weights(model.get_weights())\n",
        "def epsilon(step):\n",
        "    if step > epsilon_decay_steps: return epsilon_end\n",
        "    else: return epsilon_start - (epsilon_start - epsilon_end) * (step / epsilon_decay_steps)\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# --- DUELING NETWORK ---\n",
        "class DuelingQFunc(tf.keras.Model):\n",
        "    def __init__(self, name, action_dim):\n",
        "        super(DuelingQFunc, self).__init__(name=name)\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, (8, 8), strides=(4, 4), padding='valid', activation='relu')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='valid', activation='relu')\n",
        "        self.conv3 = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='valid', activation='relu')\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "\n",
        "        # Dueling Streams\n",
        "        self.fc1_val = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.fc2_val = tf.keras.layers.Dense(1, activation='linear') # Value stream\n",
        "\n",
        "        self.fc1_adv = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.fc2_adv = tf.keras.layers.Dense(action_dim, activation='linear') # Advantage stream\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, pixels, **kwargs):\n",
        "        pixels = tf.divide(tf.cast(pixels, tf.float32), tf.constant(255.0))\n",
        "        if len(pixels.shape) == 4 and pixels.shape[1] == 4: pixels = tf.transpose(pixels, perm=[0, 2, 3, 1])\n",
        "\n",
        "        feature = self.flat(self.conv3(self.conv2(self.conv1(pixels))))\n",
        "\n",
        "        val = self.fc2_val(self.fc1_val(feature))\n",
        "        adv = self.fc2_adv(self.fc1_adv(feature))\n",
        "\n",
        "        # Q = V + (A - mean(A))\n",
        "        return val + adv - tf.reduce_mean(adv, axis=1, keepdims=True)\n",
        "\n",
        "class DQN(object):\n",
        "    def __init__(self, action_dim):\n",
        "        self.action_dim = action_dim\n",
        "        self.qnet = DuelingQFunc('q', action_dim)\n",
        "        self.targetqnet = DuelingQFunc('targetq', action_dim)\n",
        "        dummy_obs = tf.zeros((1, 84, 84, 4))\n",
        "        self.qnet(dummy_obs); self.targetqnet(dummy_obs)\n",
        "        sync(self.qnet, self.targetqnet)\n",
        "        self.niter = 0\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=lr, epsilon=1e-5, clipnorm=clipnorm)\n",
        "        self.loss_fn = tf.keras.losses.Huber()\n",
        "\n",
        "    def get_action(self, obv):\n",
        "        if random.random() < epsilon(self.niter): return int(random.random() * self.action_dim)\n",
        "        else:\n",
        "            obv = np.expand_dims(obv, 0).astype('float32')\n",
        "            return self.qnet(obv).numpy().argmax(1)[0]\n",
        "\n",
        "    def train(self, b_o, b_a, b_r, b_o_, b_d):\n",
        "        loss = self._train_func(b_o, b_a, b_r, b_o_, b_d)\n",
        "        self.niter += 1\n",
        "        if self.niter % target_q_update_freq == 0: sync(self.qnet, self.targetqnet)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def _train_func(self, b_o, b_a, b_r, b_o_, b_d):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Double DQN Logic inside Dueling\n",
        "            b_a_ = tf.one_hot(tf.argmax(self.qnet(b_o_), 1), self.action_dim)\n",
        "            b_q_next = tf.reduce_sum(self.targetqnet(b_o_) * b_a_, 1)\n",
        "            target_q = b_r + (1 - b_d) * reward_gamma * b_q_next\n",
        "\n",
        "            b_q = tf.reduce_sum(self.qnet(b_o) * tf.one_hot(b_a, self.action_dim), 1)\n",
        "            loss = tf.reduce_mean(self.loss_fn(target_q, b_q))\n",
        "\n",
        "        grad = tape.gradient(loss, self.qnet.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))\n",
        "        return loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"Creating environment {env_id}...\")\n",
        "    try: env = build_env(env_id, seed=seed)\n",
        "    except: env = build_env('Pong-v4', seed=seed)\n",
        "\n",
        "    dqn = DQN(env.action_space.n)\n",
        "    buffer = ReplayBuffer(buffer_size)\n",
        "    o, _ = env.reset()\n",
        "\n",
        "    nepisode = 0\n",
        "    episode_reward = 0\n",
        "    loss_val = 0.0\n",
        "\n",
        "    print(\"Starting Dueling DQN Training (With Real Logs)...\")\n",
        "    for i in range(1, number_timesteps + 1):\n",
        "        a = dqn.get_action(o)\n",
        "        o_, r, terminated, truncated, info = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        buffer.add(o, a, r, o_, done)\n",
        "        episode_reward += r\n",
        "\n",
        "        if i >= warm_start and i % train_freq == 0:\n",
        "            transitions = buffer.sample(batch_size)\n",
        "            # اینجا مقدار لاس را می‌گیریم\n",
        "            loss_val = dqn.train(*transitions)\n",
        "\n",
        "        # نمایش لاگ واقعی\n",
        "        if i % 100 == 0:\n",
        "             loss_str = f\"{loss_val:.4f}\" if i > warm_start else \"Collecting Data\"\n",
        "             print(f\"Step: {i} / {number_timesteps} | Loss: {loss_str} | Epsilon: {epsilon(i):.3f}\")\n",
        "\n",
        "        if done:\n",
        "            nepisode += 1\n",
        "            print(f\"*** EPISODE {nepisode} DONE *** Reward: {episode_reward} | Step: {i}\")\n",
        "            episode_reward = 0\n",
        "            o, _ = env.reset()\n",
        "        else:\n",
        "            o = o_\n",
        "\n",
        "    print(\"Dueling DQN Finished.\")"
      ]
    }
  ]
}